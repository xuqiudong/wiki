---
title: java并发编程基础
description: 
published: true
date: 2023-07-25T08:13:22.807Z
tags: 咕泡, 并发, 面试
editor: markdown
dateCreated: 2023-06-20T08:09:20.098Z
---

# 01 谈谈你对AQS的理解  TODO
1. AQS 是多线程同步器，它是 J.U.C 包中多个组件的底层实现，如 Lock、
CountDownLatch、Semaphore 等都用到了 AQS.
2. 从本质上来说，AQS 提供了两种锁机制，分别是排它锁，和 共享锁。
 - 排它锁，就是存在多线程竞争同一共享资源时，同一时刻只允许一个线程访问该共享资源，也就是多个线程中只能有一个线程获得锁资源，比如 Lock 中的ReentrantLock 重入锁实现就是用到了 AQS 中的排它锁功能。
 - 共享锁也称为读锁，就是在同一时刻允许多个线程同时获得锁资源，比如
CountDownLatch 和 Semaphore 都是用到了 AQS 中的共享锁功能。

# 02 lock 和 synchronized 区别
1. 从功能角度来看，Lock 和 Synchronized 都是 Java 中用来解决线程安全问题的工具。
2. 从特性来看，
	- a. Synchronized 是 Java 中的同步关键字，Lock 是 J.U.C 包中提供的接口，这个接口有很多实现类，其中就包括 ReentrantLock 重入锁。
  - b. Synchronized 可以通过两种方式来控制锁的粒度。一种是把 synchronized 关键字修饰在方法层面，另一种是修饰在代码块上，并且我们可以通过 Synchronized 加锁对象的声明周期来控。
   - 制锁的作用范围，比如锁对象是静态对象或者类对象，那么这个锁就是全局锁。
   - 如果锁对象是普通实例对象，那这个锁的范围取决于这个实例的声明周期。
   - Lock 锁的粒度是通过它里面提供的 lock()和 unlock()方法决定的（贴图），包裹在这两个方法之间的代码能够保证线程安全性。而锁的作用域取决于 Lock 实例的生命周期。
  - c. Lock 比 Synchronized 的灵活性更高，Lock 可以自主决定什么时候加锁，什么时候释放锁，只需要调用 lock()和 unlock()这两个方法就行，同时 Lock 还提供了非阻塞的竞争锁方法 tryLock()方法，这个方法通过返回 true/false 来告诉当前线程是否已经有其他线程正在使用锁。 Synchronized 由于是关键字，所以它无法实现非阻塞竞争锁的方法，另外，Synchronized 锁的释放是被动的，就是当 Synchronized 同步代码块执行完以后或者代码出现异常时才会释放    
  - d.  Lock 提供了公平锁和非公平锁的机制，公平锁是指线程竞争锁资源时，如果已经有其他线程正在排队等待锁释放，那么当前竞争锁资源的线程无法插队。而非公平锁，就是不管是否有线程在排队等待锁，它都会尝试去竞争一次锁。Synchronized 只提供了一种非公平锁的实现.(公平锁老实在队列里呆着，非公平锁会先竞争一次锁，再进入队列)，
3.   从性能方面来看，Synchronized 和 Lock 在性能方面相差不大，在实现上会有一些区别，Synchronized 引入了偏向锁、轻量级锁、重量级锁以及锁升级的方式来优化加锁的性能，而 Lock 中则用到了自旋锁的方式来实现性能优化。
  
# 03 线程池如何知道一个线程的任务已经执行完成 
1. 在线程池内部，当我们把一个任务丢给线程池去执行，线程池会调度工作线程来执行这个任务的 run 方法，run 方法正常结束，也就意味着任务完成了。所以线程池中的工作线程是通过同步调用任务的 run()方法并且等待 run 方法返回后，再去统计任务的完成数量。
2. 如果想在线程池外部去获得线程池内部任务的执行状态，有几种方法可以实现。
  - a. 线程池提供了一个 isTerminated()方法，可以判断线程池的运行状态，我们可以循环判断 isTerminated()方法的返回结果来了解线程池的运行状态，一旦线程池的运行状态是 Terminated，意味着线程池中的所有任务都已经执行完了。想要通过这个方法获取状态的前提是，程序中主动调用了线程池的 shutdown()方法。在实际业务中，一般不会主动去关闭线程池，因此这个方法在实用性和灵活性方面都不是很好
  - b.  在线程池中，有一个 submit()方法，它提供了一个 Future 的返回值，我们通过 Future.get()方法来获得任务的执行结果，当线程池中的任务没执行完之前，future.get()方法会一直阻塞，直到任务执行结束。因此，只要 future.get()方法正常返回，也就意味着传入到线程池中的任务已经执行完成了！(execute方法返回是void )
  - c. 可以引入一个 CountDownLatch 计数器，它可以通过初始化指定一个计数器进行倒计时，其中有两个方法分别是 await()阻塞线程，以及 countDown()进行倒计时，一旦倒计时归零，所以被阻塞在 await()方法的线程都会被释放。基于这样的原理，我们可以定义一个 CountDownLatch 对象并且计数器为 1，接着在线程池代码块后面调用 await()方法阻塞主线程，然后，当传入到线程池中的任务执行完成后，调用 countDown()方法表示任务执行结束。最后，计数器归零 0，唤醒阻塞在 await()方法的线程。
  
# 04 什么叫做阻塞队列的有界和无界  
1. 阻塞队列，是一种特殊的队列，它在普通队列的基础上提供了两个附加功能
  - a. 当队列为空的时候，获取队列中元素的消费者线程会被阻塞，同时唤醒生产者线程。
  - b. 当队列满了的时候，向队列中添加元素的生产者线程被阻塞，同时唤醒消费者线程。
2. 其中，阻塞队列中能够容纳的元素个数，通常情况下是有界的，比如我们实例化一个 ArrayBlockingList，可以在构造方法中传入一个整形的数字，表示这个基于数组的阻塞队列中能够容纳的元素个数。这种就是有界队列。  
3. 而无界队列，就是没有设置固定大小的队列，不过它并不是像我们理解的那种元素没有任何限制，而是它的元素存储量很大，像 LinkedBlockingQueue，它的默认队列长度是 Integer.Max_Value，所以我们感知不到它的长度限制。
4. 无界队列存在比较大的潜在风险，如果在并发量较大的情况下，线程池中可以几乎无限制的添加任务，容易导致内存溢出的问题！

# 05 ConcurrentHashMap 底层具体实现？实现原理是什么
1. **整体结构**： ConcurrentHashMap 在 JDK1.8 中的存储结构，它由数组、单向链表、红黑树组成。  
  - 默认初始化一个长度为16的数组
  - 核心是hash表，依然存在hash冲突，ConcurrentHashMap采用链式寻址法来解决 hash 冲突。
  - 当 hash 冲突比较多的时候，会造成链表长度较长，这种情况会使得ConcurrentHashMap 中数据元素的查询复杂度变成 O(n)。因此在 JDK1.8 中，引入了红黑树的机制。
  - 当数组长度大于 64 并且链表长度大于等于 8 的时候，单项链表就会转换为红黑树。
  - 随着 ConcurrentHashMap 的动态扩容，一旦链表长度小于 8，红黑树会退化成单向链表。
2. **基本功能**：
 - 本质上是一个 HashMap，因此功能和 HashMap 一样，但是ConcurrentHashMap 在 HashMap 的基础上，提供了并发安全的实现。
 - 并发安全的主要实现是通过对指定的 Node 节点加锁，来保证数据更新的安全性。
 - ConcurrentHashMap 在性能方面做的优化：
 > 如何在并发性能和数据安全性之间做好平衡，在很多地方都有类似的设计，比如 cpu的三级缓存、mysql 的 buffer_pool、Synchronized 的锁升级等等。
 
  - 1. 在 JDK1.8 中，ConcurrentHashMap 锁的粒度是数组中的某一个节点，而在JDK1.7，锁定的是 Segment，锁的范围要更大，因此性能上会更低。
  - 2. 引入红黑树，降低了数据查询的时间复杂度，红黑树的时间复杂度是 O(logn)
  - 3. 当数组长度不够时，ConcurrentHashMap 需要对数组进行扩
容，在扩容的实现上，ConcurrentHashMap 引入了多线程并发扩容的机制，简单来说就是多个线程对原始数组进行分片后，每个线程负责一个分片的数据迁移，从而提升了扩容过程中数据迁移的效率。
  - 4. ConcurrentHashMap 中有一个 size()方法来获取总的元素个数，而在多线程并发场景中，在保证原子性的前提下来实现元素个数的累加，性能是非常低的。ConcurrentHashMap 在这个方面的优化主要体现在两个点： 
    a.  当线程竞争不激烈时，直接采用 CAS 来实现元素个数的原子递增。
    b. 如果线程竞争激烈，使用一个数组来维护元素个数，如果要增加总的元素个数，则直接从数组中随机选择一个，再通过 CAS 实现原子递增。它的核心思想是引入了数组来实现对并发更新的负载。
    
# 06 CAS机制
- CAS 是 Java 中 Unsafe 类里面的方法，它的全称是 CompareAndSwap，比较并交换的意思。它的主要功能是能够保证在多线程环境下，对于共享变量的修改的原子性。
- CompareAndSwap 的底层实现中，在多核 CPU 环境下，会增加一个 Lock指令对缓存或者总线加锁，从而保证比较并替换这两个指令的原子性。
- CAS 主要用在并发场景中，比较典型的使用场景有两个。
  1. J.U.C 里面 Atomic 的原子实现，比如 AtomicInteger，AtomicLong。
  2. 实现多线程对共享资源竞争的互斥性质，比如在 AQS、ConcurrentHashMap、ConcurrentLinkedQueue 等都有用到。
  
# 07 死锁的发生原因和怎么避免
- 原因：多线程下对共享资源竞争造成相互等待的现象。
 - 1. 互斥条件
 - 2. 不可剥夺条件
 - 3. 请求和保持条件： 不释放资源
 - 4. 循环等待条件
- 死锁后，只能通过人工干预来解决，比如重启，或kill进程。
- 对于请求和保持：一次性申请全部的资源
- 不可剥夺：申请资源失败，可主动释放它占有的资源
- 循环等待：按序申请资源，先申请小的，再申请大的。

# 08 volatile关键字的作用和实现原理
- 作用：
	1. 保证多线程下共享变量的可见性
  2. 增加内存屏障防止多个智力之间的重排序

- 可见性：一个线程修改共享变量后，其他线程立刻可以看到修改后的只。
- **造成可见性问题的原因**：
 1. **CPU 层面的高速缓存**：对于增加了 volatile 关键字修饰的共享变量，JVM 虚拟机会自动增加一个#Lock汇编指令，这个指令会根据 CPU 型号自动添加总线锁或/缓存锁
   - 1.1  总线锁： 锁定了 CPU 的前端总线，从而导致在同一时刻只能有一个线程去和内存通信，这样就避免了多线程并发造成的可见性。
   - 1.2  缓存锁是对总线锁的优化，因为总线锁导致了 CPU 的使用效率大幅度下降，所以缓存锁只针对 CPU 三级缓存中的目标数据加锁，缓存锁是使用 MESI 缓存一致性来实现的
 2. **指令重排序**：  所谓重排序，就是指令的编写顺序和执行顺序不一致，在多线程环境下导致可见性问题。指令重排序本质上是一种性能优化的手段，它来自于几个方面：
  - 2.1 CPU 层面，针对 MESI 协议的更进一步优化去提升 CPU 的利用率，引入了StoreBuffer 机制，而这一种优化机制会导致 CPU 的乱序执行。当然为了避免这样的问题，CPU 提供了**内存屏障指令**，上层应用可以在合适的地方插入内存屏障来避免 CPU 指令重排序问题。
  - 2.2 编译器的优化，编译器在编译的过程中，在不改变单线程语义和程序正确性的前提下，对指令进行合理的重排序优化来提升性能。 所以，如果对共享变量增加了 volatile 关键字，那么在编译器层面，就不会去触发编译器优化，同时再 JVM 里面，会插入内存屏障指令来避免重排序问题。
  
- 除了 volatile 以外，从 JDK5 开始，JMM 就使用了一种 Happens-Before 模型去描述多线程之间的内存可见性问题。如果两个操作之间具备 Happens-Before 关系，那么意味着这两个操作具备可见性关系，不需要再额外去考虑增加 volatile 关键字来提供可见性保障。  

# 09 wait 和 notify为什么要在synchronized 代码块中
> **实现 wait/notify 机制的条件**：
调用 wait 线程和 notify 线程必须拥有相同对象锁。
wait() 方法和 notify()/notifyAll() 方法必须在 Synchronized 方法或代码块中。
1. wait 和 notify 用来实现多线程之间的协调，wait 表示让线程进入到阻塞状态，notify 表示让阻塞的线程唤醒。
2. 二者必须成对出现，从而实现多线程之间的通信。
3.  Synchronized 可以实现一个等待一个唤醒这样的互斥条件
4. 为了避免 wait/notify 的错误使用，jdk 强制要求把 wait/notify 写在同步代码块里面，否则会抛出 IllegalMonitorStateException
5. wait/notify 的特性，非常适合实现生产者消费者的模型

# 10 ThreadLocal的实现原理
1. ThreadLocal 是一种线程隔离机制，提供多线程环境下对于共享变量的安全访问。
2. ThreadLocal 用了一种空间换时间的设计思想，也就是说在每个线程里面，都有一个容器来存储共享变量的副本，然后每个线程只对自己的变量副本来做更新操作，这样既解决了线程安全问题，又避免了多线程竞争加锁的开销。
3. ThreadLocal 原理是，在 Thread 类里面有一个成员变量ThreadLocalMap，它专门来存储当前线程的共享变量副本，后续这个线程对于共享变量的操作，都是从这个 ThreadLocalMap 里面进行变更，不会影响全局共享变量的值。
> 在 ThreadLocal 中，除了空间换时间的设计思想以外，还有一些比较好的设计思想，比如线性探索解决 hash 冲突，数据预清理机制、弱引用 key 设计尽可能避免内存泄漏等。

# 11 基于数组的阻塞队列 ArrayBlockingQueue 原理
1. 阻塞队列（BlockingQueue）是在队列的基础上增加了两个附加操作:
  - 1.1  在队列为空的时候，获取元素的线程会等待队列变为非空。
  - 1.2   当队列满时，存储元素的线程会等待队列可用。
2. 由于阻塞队列的特性，可以非常容易实现生产者消费者模型，也就是生产者只需要关心数据的生产，消费者只需要关注数据的消费，所以如果队列满了，生产者就等待，同样，队列空了，消费者也需要等待。 
3. 要实现这样的一个阻塞队列，需要用到两个关键的技术，队列元素的存储、以及线程阻塞和唤醒
4. 而 ArrayBlockingQueue 是基于数组结构的阻塞队列，也就是队列元素是存储在一个数组结构里面，并且由于数组有长度限制，为了达到循环生产和循环消费的目的，ArrayBlockingQueue 用到了循环数组。
5. 而线程的阻塞和唤醒，用到了 J.U.C 包里面的 ReentrantLock 和 Condition。Condition 相当于 wait/notify 在 JUC 包里面的实现。

# 12 怎么理解线程安全
1. 在多个线程访问某个方法或者对象的时候，不管通过任何的方式调用以及线程如何去交替执行。在程序中不做任何同步干预操作的情况下，这个方法或者对象的执行/修改都能按照预期的结果来反馈，那么这个类就是线程安全的。
2. 线程安全体现在三个方面：原子性、有序性、可见性。
 - a. 原子性：一个线程执行一系列程序指令操作的时候，它应该是不可中断的，就是一段程序只能由一个线程完整的执行完成，而不能存在多个线程干扰。 主要是由于CPU上线问切换引起的。Synchronized 关键字可解决原子性问题
 - b. 可见性：某个线程对共享变量的修改，对其他线程不是实时可见的。原因可能是：CPU 的高速缓存、CPU 的指令重排序、编译器的指令重排序。
 - c. 有序性：程序编写的指令顺序和最终 CPU 运行的指令顺序可能出现不一致的现象，即指令重排序。它也会导致可见性问题。Volatile 关键字来解决。
3. 导致有序性、原子性、可见性问题的本质，是为了最大化提升CPU 利用率导致的。比如为了提升 CPU 利用率，设计了三级缓存、设计了 StoreBuffer、设计了缓存行这种预读机制、在操作系统里面，设计了线程模型、在编译器里面，设计了编译器的深度优化机制。 

# 13 什么是可重入，什么是可重入锁? 它用来解决什么问题?
1. 一个线程如果抢占到了互斥锁资源，在锁释放之前再去竞争同一把锁的时候，不需要等待，只需要记录重入次数。
2. Synchronized、ReentrantLock 等可重入。读写锁StampedLock不可重入。
3. 锁的可重入性，主要解决的问题是**避免线程死锁的问题**。  因为一个已经获得同步锁 X 的线程，在释放锁 X 之前再去竞争锁 X 的时候，相当于会出现自己要等待自己释放锁，这很显然是无法成立的。

# 14 ReentrantLock 的实现原理
1. ReentrantLock 是一种可重入的排它锁，主要用来解决多线程对共享资源竞争的问题。
2. 特性：
	- 2.1  支持可重入 ： 获得锁的线程在释放锁之前再次去竞争同一把锁的时候，不需要加锁就可以直接访问
  - 2.2  支持公平和非公平特性
  - 2.3  提供了阻塞竞争锁和非阻塞竞争锁的两种方法，分别是 lock()和 tryLock()。
3. 底层实现：
  - 3.1  锁的竞争： 通过互斥变量，使用 CAS 机制来实现的
  - 3.2  没有竞争到锁的线程，使用了 AbstractQueuedSynchronizer 存储，底层是通过双向链表来实现的。当锁被释放之后，会从 AQS 队列里面的头部唤醒下一个等待锁的线程。
  - 3.3 公平和非公平的特性，主要是体现在竞争锁的时候，是否需要判断 AQS 队列存在等待中的线程
  - 3.4 锁的重入特性，在 AQS 里面有一个成员变量来保存当前获得锁的线程，当同一个线程下次再来竞争锁的时候，就不会去走锁竞争的逻辑，而是直接增加重入次数.
  
# 15 简述对线程池的理解
    线程池本质上是一种池化技术，是一种资源复用的思想。比如常见的连接池、内存池、对象池。
- 线程池复用的是线程资源，它的核心实际目标有两个：
1. 减少线程的频繁创建和销毁带来的性能开销：线程创建会涉及到 CPU 上下文切换、内存分配等工作；
2. 线程池本身会有参数来控制线程创建的数量，这样就可以避免无休止的创建线程带来的资源利用率过高的问题
- 线程池中的线程的生命周期是由任务运行的状态决定的，无法人为控制：
- 为了实现线程的复用，线程池里面用到了阻塞队列，简单来说就是线程池里面的工作线程处于一直运行状态，它会从阻塞队列中去获取待执行的任务，一旦队列空了，那这个工作线程就会被阻塞，直到下次有新的任务进来。
  
# 16 为什么引入偏向锁、轻量级锁，升级流程是什么
1. Synchronized 在 jdk1.6 版本之前，是通过重量级锁的方式来实现线程之间锁的竞争。底层底层依赖操作系统的 Mutex Lock 来实现互斥功能。
2. 在 jdk1.6 版本中，synchronized 增加了锁升级的机制，来平衡数据安全性和性能：
  - 2.1 线程去访问 synchronized 同步代码块的时候，synchronized 根据线程竞争情况，会先尝试在不加重量级锁的情况下去保证线程安全性。所以引入了偏向锁和轻量级锁的机制。
  - 2.2 偏向锁，就是直接把当前锁偏向于某个线程，简单来说就是通过 CAS 修改偏向锁标记，这种锁适合同一个线程多次去申请同一个锁资源并且没有其他线程竞争的场景。
  - 2.3 轻量级锁也可以称为自旋锁，基于自适应自旋的机制，通过多次自旋重试去竞争锁。自旋锁优点在于它避免避免了用户态到内核态的切换带来的性能开销。